---
title: "Case Study 4 in 3d"
format: html
editor: 
  markdown: 
    wrap: 72
---

New package `GGally` for interactive scatterplot matrixes.

```{r}
library(tidyverse)
library(DescTools)
library(GGally)
library(plotly)
```

# ACED Data

```{r loadACEDnoNA}
ACEDextract <- read_csv("ACED_extract1.csv",na="-999")
ACEDextract$Session <- factor(ACEDextract$Session)
ACEDextract$Cond_code <- factor(ACEDextract$Cond_code)
ACEDextract$Sequencing <- factor(ACEDextract$Sequencing)
ACEDextract$Feedback <- factor(ACEDextract$Feedback)
ACEDextract$Gender <- factor(ACEDextract$Gender)
ACEDextract$Race <- factor(ACEDextract$Race,1:8)
ACEDextract$Level_Code <- factor(ACEDextract$Level_Code)
```

```{r mutate}
ACEDextract %>%
  mutate(gain=post_scaled-pre_scaled) ->
  ACEDextract
```

```{r ACEDskills}
ACED.skillNames <- 
  structure(list(long = c("solveGeometricProblems", "commonRatio", 
  "examplesGeometric", "explicitGeometric", "extendGeometric", 
  "modelGeometric", "recursiveRuleGeometric", "tableGeometric", 
  "verbalGeometric", "visualGeometric", "determineTypes"), short = c("sgp", 
  "cr", "eg", "exp", "ext", "mod", "rr", "tab", "vr", "pic", "dt"
  )), class = "data.frame", row.names = c(NA, -11L))
ACED.skillNames
```

```{r}
mathmarks <- read.csv("mathmarks.csv")
summary(mathmarks)
```

# A little bit of Matrix Algebra

## Column Vectors

$$\boldsymbol{y} = 
 \left [ \begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_N
 \end{array} \right ]$$

## Model Matrix

Rows are subjects and columns are variables.

$$\textbf{X} =
\left [
\begin{array}{cccc}
x_{11} & x_{12} & \ldots & x_{1p} \\
x_{21} & x_{22} & \ldots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{N1} & x_{N2} & \ldots & x_{Np} 
\end{array} \right ]$$

## Transpose

Interchange rows and columns. (Turns row vector into a column vector).

If $\textbf{X}$ is a $N\times p$ matrix, $\textbf{X}^T$ (also written
$\textbf{X}'$) is a $p\times N$ matrix.

$$\textbf{X}^T =
\left [
\begin{array}{cccc}
x_{11} & x_{12} & \ldots & x_{1N} \\
x_{21} & x_{22} & \ldots & x_{2N} \\
\vdots & \vdots & \ddots & \vdots \\
x_{p1} & x_{p2} & \ldots & x_{pN} 
\end{array} \right ]$$

## Matrix Multiplication

Let $\textbf{X}$ be a $J\times N$ matrix and $\textbf{Z}$ be a
$N\times K$ matrix. The product is a $J \times K$ matrix.

$$\textbf{X}\textbf{Z} = \left [
\begin{array}{cccc}
\sum x_{1i}z_{i1} & \sum x_{1i}z_{i2} & \ldots & \sum x_{1i}z_{iK} \\
\sum x_{2i}z_{i1} & \sum x_{2i}z_{i2} & \ldots & \sum x_{2i}z_{iK} \\
\vdots & \vdots & \ddots & \vdots \\
\sum x_{Ji}z_{i1} & \sum x_{Ji}z_{i2} & \ldots & \sum x_{Ji}z_{iK} 
\end{array} \right ]$$

The inner dimension must match.

In R, use `%*%` to indicate matrix multiplication. (`*` is element-wise
multiplication.)

## Regression Equation (matrix form)

Let $\textbf{X}$ be the $N \times p$ matrix of explanatory variables.
Let $\boldsymbol{y}$ be the vector of response variables.

Add a column of 1's to the left side of $\textbf{X}$, so it is now a
$N \times p+1$ matrix.

Let $\boldsymbol{b}^T = (b_0, b_1, \ldots, b_p)$

$$\textbf{X}\boldsymbol{b} = \left [
\begin{array}{c}
b_0(1) + b_1 x_{11} + \cdots + b_p x_{1p} \\
b_0(1) + b_1 x_{21} + \cdots + b_p x_{2p} \\
\vdots \\
b_0(1) + b_1 x_{N1} + \cdots + b_p x_{Np}
\end{array} \right ]
$$

$$\boldsymbol{y} = \textbf{X}\boldsymbol{b} + \boldsymbol{e}$$

where $\boldsymbol{e} \sim N(\boldsymbol{0},\sigma_{e}\textbf{I})$
$\sigma_e$ is the residual variance.

## Solving the regression Equation

$$\textbf{X}\boldsymbol{b} = \boldsymbol{y}$$

$$\textbf{X}^T\textbf{X}\boldsymbol{b} = \textbf{X}^T\boldsymbol{y}$$
$$(\textbf{X}^T\textbf{X})^{-1}(\textbf{X}^T\textbf{X})\boldsymbol{b} =
(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\boldsymbol{y}$$

$$\boldsymbol{b} =
(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\boldsymbol{y}$$

Note: Need to invert $(\textbf{X}^T\textbf{X})$. R uses $\textbf{QR}$
decomposition to do this.

# Multivariate normal distribution

Mean is a vector $\boldsymbol{\mu}^T=(\mu_1, \mu_2, \ldots, \mu_K)$

Variance is a matrix

## Covariance Matrix

$$ \boldsymbol{\Sigma} = \left [
\begin{array}{cccc}
\sigma_{11} & \sigma_{12} & \cdots & \sigma_{1K} \\
\sigma_{21} & \sigma_{22} & \cdots & \sigma_{2K} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{K1} & \sigma_{K2} & \cdots & \sigma_{KK}
\end{array} \right ]$$

The elements are *covariances*
$$\sigma_{ij} = E\left [(X_i-\mu_i)(X_j-\mu_j)\right ]$$ Diagonal
elements, $\sigma_{kk}$, are variances.

$\Sigma$ is symmetric $\Sigma$ is positive semi-definite:\
$\forall \boldsymbol{x}: \boldsymbol{x}^T\Sigma\boldsymbol{x} \ge 0$

## Multivariate Normal Distribution

$$f(\boldsymbol{x}) = \frac{1}{\sqrt{(2\pi)^p \textrm{det}(\Sigma)}}
\exp\left ( -\frac{1}{2}
(\boldsymbol{x}-\boldsymbol{\mu})^T \Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})
\right )$$

## Estimating the Covariance Matrix

If mean is known: $$\hat \Sigma = (\textbf{X}-\mu)^T(\textbf{X}-\mu)/N$$
If mean is unknown:

$$\hat \Sigma = (\textbf{X}-\bar{\boldsymbol{x}})^T(\textbf{X}-\bar{\boldsymbol{x}})/(N-1)$$

$$ (\textbf{X}-\bar{\boldsymbol{x}})^T(\textbf{X}-\bar{\boldsymbol{x}}) =
\left [\begin{array}{cccc}
\sum (x_{11}-\bar{x_1})^2 & \sum (x_{11}-\bar{x_1})(x_{12}-\bar{x_2}) & \ldots &
\sum (x_{11}-\bar{x_1})(x_{1p}-\bar{x_p}) \\
\sum (x_{12}-\bar{x_2})(x_{11}-\bar{x_1}) & \sum (x_{12}-\bar{x_2})^2 & \ldots &
\sum (x_{12}-\bar{x_2})(x_{1p}-\bar{x_p}) \\
\vdots & \vdots & \ddots & \vdots \\
\sum (x_{1p}-\bar{x_p})(x_{11}-\bar{x_1}) & \sum (x_{1p}-\bar{x_p})(x_{12}-\bar{x_2}) & \ldots &
\sum (x_{1p}-\bar{x_p})^2
\end{array}\right]$$

## Correlation matrix

Define a matrix with the standard deviations along the diagonal.
$$\text{diag}(\boldsymbol{\sigma})=
\left [\begin{array}{cccc}
\sqrt{\sigma_{11}} & 0 & \ldots & 0 \\
0 & \sqrt{\sigma_{22}} & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \sqrt{\sigma_{22}} 
\end{array} \right ]$$

Then
$\boldsymbol{\Sigma}=\text{diag}(\boldsymbol{\sigma})\boldsymbol{P}\text{diag}(\boldsymbol{\sigma})$

Where $\rho_{ij}$ is the correlation between $X_i$ and $X_j$
($\rho_{ii}=1$).

The `diag()` function in R can be used to both get the diagonal of a
square matrix and turn a vector into a square matrix with that vector on
the diagonal (and zeros elsewhere).

# Correlation and Scatterplot Matrixes

## Calculating Correlation Matrixes

```{r}
round(cov(mathmarks),3)
```

```{r}
round(cor(mathmarks),3)
```

```{r}
select(ACEDextract,starts_with("EAP")) ->ACEDeaps
round(cor(ACEDeaps,use="complete.obs"),3)
```

## Scatterplot matrix

```{r scatmat}
highlight_key(mathmarks) %>%
  GGally::ggpairs(columns=1:5) %>%
  ggplotly() %>%
  highlight("plotly_selected")
```

```{r 3d}
plot_ly(mathmarks, x=~ALG, y=~ANL, z=~STAT)
```

```{r ACEDscatmat}
highlight_key(ACEDeaps) %>%
  GGally::ggpairs(columns=1:4) %>%
  ggplotly() %>%
  highlight("plotly_selected")
```

# Linear Models

`Y ~ X1 + X2 +`...

Difference in models

`. ~ . + X3`

`. ~ . - X1`

No constant model `. ~ . - 1`

## Initial Model

```{r mod1}
mod1 <- lm(post_scaled ~ pre_scaled, data=ACEDextract, na.action = na.omit)
summary(mod1)
```

## Update

```{r mod2}
mod2 <- update(mod1,.~.+EAP.sgp)
summary(mod2)
```

## Model Differences

```{r diff}
round(summary(mod2)$r.squared - summary(mod1)$r.squared,3)
```

## Model ANOVA

```{r anova}
try(anova(mod1,mod2))
```

```{r anova1}
filter(ACEDextract,!is.na(pre_scaled) & !is.na(EAP.sgp)) -> dat1
lm(post_scaled ~ pre_scaled, data=dat1) -> mod1
mod2 <- update(mod1,.~.+EAP.sgp)
anova(mod1,mod2)
```

# Colinearity and Singular Matrixes

## Aliasing and Singular Matrixes

```{r}
lm(post_scaled~P.sgp..H + P.sgp..M + P.sgp..L, data=ACEDextract,
   na.action=na.omit)
```

```{r}
select(ACEDextract,starts_with("P.sgp")) %>% rowSums() %>% head()
```

This is like division by zero.

```{r}
plot_ly(ACEDextract, x= ~ P.sgp..H, y= ~ P.sgp..M, z=~P.sgp..L)
```

## Colinearity, approximately singular matrix.

If a linear combination of variables is close to 1, then we have
approximately division by 0.

Suppose $X_1$ and $X_2$ are highly correlated

Then $$X_1 + X_2 \approx (1-c)X_1 + (1+c)X_2$$

## Fixing colinearity

1.  Drop one or more variables.

2.  Replace colinear variables by (weighted) average or sum

# Interactions, Factors and Contrasts

## Interaction Terms

`X1 + X2 + X1:X2` or `X1*X2`

Adds `X1*X2` as a new column in the model matrix.

## Factors and Dummy Variables

Remap logical variable to `TRUE=1` and `FALSE=0`.

Any binary factor, pick one category for 1 and one for 0.

For a factor with $k$ states, could add $k$ columns: but this produces a
singular matrix.

## Contrasts

1.  *Treatment*: Pick one state as the baseline, and compare all other
    to that.

-   Default for unordered factors
-   Can select which state is the baseline (control)
-   Note: SPSS has changed from first to last in different versions
-   *One Hot Encoding* does not drop the one column (`contrasts=FALSE`)

2.  *Sum*: Each column compares one state against the last

3.  *Polynomial*: Linear, Quadratic, Cubic, ...

-   Default for ordered factors

4.  *Helmert*: Alternative, compares 1 vs 2, ave(1 and 2) vs 3, ave(1:3)
    vs 4

```{r}
help(contrasts)
```

Usually default just works. In a formula use `C()` function to override
the default.

## model.frame and model.matrix

1.  `model.frame` -- pull out variables referenced in formula
2.  `na.action` -- remove rows with NAs
3.  `model.matrix` -- dummy code factor variables, calculate
    interactions, add constant column
4.  Fit the model (QR decomposition)
5.  Package result up into model object
